{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the 3 sections in **bold** above. In the next lab, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-98e4-2df4-fac3-8bc7.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-98e4-2df4-fac3-8bc7.qdrep\"\n",
      "Exporting 8466 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-98e4-2df4-fac3-8bc7.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   62.8       245444059           3      81814686.3           18871       245349222  cudaMallocManaged                                                               \n",
      "   31.6       123501357           1     123501357.0       123501357       123501357  cudaDeviceSynchronize                                                           \n",
      "    5.5        21648539           3       7216179.7         6529443         8422104  cudaFree                                                                        \n",
      "    0.0           50221           1         50221.0           50221           50221  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       123490089           1     123490089.0       123490089       123490089  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.2        76240640        6508         11714.9            2176          164480  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   21.8        21207264         768         27613.6            1536          160768  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            6508               60.420              4.000              984.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.5      1505912526          80      18823906.6           22528       100134955  poll                                                                            \n",
      "   41.1      1137263445          80      14215793.1           15055       100121320  sem_timedwait                                                                   \n",
      "    3.5        96269659         664        144984.4            1107        17220169  ioctl                                                                           \n",
      "    0.9        23838818          91        261965.0            1301         8341576  mmap                                                                            \n",
      "    0.1         1482142          77         19248.6            4960           33365  open64                                                                          \n",
      "    0.0          173648           4         43412.0           32388           51221  pthread_create                                                                  \n",
      "    0.0          166657           3         55552.3           52995           59976  fgets                                                                           \n",
      "    0.0          127374          25          5095.0            1542           23288  fopen                                                                           \n",
      "    0.0           87281          11          7934.6            3813           12589  write                                                                           \n",
      "    0.0           57480          13          4421.5            1902            7163  munmap                                                                          \n",
      "    0.0           40034           3         13344.7           11952           14322  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           36865           5          7373.0            3647            9466  open                                                                            \n",
      "    0.0           27852          18          1547.3            1038            4356  fclose                                                                          \n",
      "    0.0           25038          19          1317.8            1002            4429  fcntl                                                                           \n",
      "    0.0           23305          13          1792.7            1137            2745  read                                                                            \n",
      "    0.0           16392           2          8196.0            6961            9431  socket                                                                          \n",
      "    0.0           12213           1         12213.0           12213           12213  sem_wait                                                                        \n",
      "    0.0           11687           1         11687.0           11687           11687  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            9696           3          3232.0            1841            3978  fread                                                                           \n",
      "    0.0            8652           4          2163.0            1921            2604  mprotect                                                                        \n",
      "    0.0            8588           1          8588.0            8588            8588  connect                                                                         \n",
      "    0.0            7959           1          7959.0            7959            7959  pipe2                                                                           \n",
      "    0.0            2955           1          2955.0            2955            2955  bind                                                                            \n",
      "    0.0            1675           1          1675.0            1675            1675  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report6.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report6.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-f88d-1480-a171-fc06.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-f88d-1480-a171-fc06.qdrep\"\n",
      "Exporting 1859 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-f88d-1480-a171-fc06.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   88.8       234652308           1     234652308.0       234652308       234652308  cudaMallocManaged                                                               \n",
      "    7.8        20535244           1      20535244.0        20535244        20535244  cudaDeviceSynchronize                                                           \n",
      "    3.4         8881627           1       8881627.0         8881627         8881627  cudaFree                                                                        \n",
      "    0.0           72588           1         72588.0           72588           72588  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0        20527066           1      20527066.0        20527066        20527066  deviceKernel(int*, int)                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21358944         768         27811.1            1600          173056  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   50.5       742302278          40      18557556.9           27181       100136622  poll                                                                            \n",
      "   42.8       628362506          39      16111859.1           10474       100083106  sem_timedwait                                                                   \n",
      "    5.7        84197563         650        129534.7            1019        18648346  ioctl                                                                           \n",
      "    0.8        12072480          85        142029.2            1504         8722783  mmap                                                                            \n",
      "    0.1         1688050          77         21922.7            8480           42807  open64                                                                          \n",
      "    0.0          227783           4         56945.7           52425           61945  pthread_create                                                                  \n",
      "    0.0          209019           3         69673.0           68547           71609  fgets                                                                           \n",
      "    0.0          181537          25          7261.5            2138           28685  fopen                                                                           \n",
      "    0.0          104530          11          9502.7            4520           18375  write                                                                           \n",
      "    0.0           51489           5         10297.8            6372           13598  open                                                                            \n",
      "    0.0           48576          11          4416.0            1435            8654  munmap                                                                          \n",
      "    0.0           42577           4         10644.2            1057           18167  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           36911          28          1318.3            1002            4953  fcntl                                                                           \n",
      "    0.0           35993          18          1999.6            1260            5708  fclose                                                                          \n",
      "    0.0           26805          13          2061.9            1076            4863  read                                                                            \n",
      "    0.0           20500           2         10250.0            7880           12620  socket                                                                          \n",
      "    0.0           12267           1         12267.0           12267           12267  connect                                                                         \n",
      "    0.0            9914           4          2478.5            2242            2679  mprotect                                                                        \n",
      "    0.0            7675           2          3837.5            3584            4091  fread                                                                           \n",
      "    0.0            7538           1          7538.0            7538            7538  pipe2                                                                           \n",
      "    0.0            5755           1          5755.0            5755            5755  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            2853           1          2853.0            2853            2853  bind                                                                            \n",
      "    0.0            1651           1          1651.0            1651            1651  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report13.qdrep\"\n",
      "Report file moved to \"/dli/task/report13.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-4b21-addd-5e0e-beed.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-4b21-addd-5e0e-beed.qdrep\"\n",
      "Exporting 8631 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-4b21-addd-5e0e-beed.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   60.8       249950731           3      83316910.3           18148       249857504  cudaMallocManaged                                                               \n",
      "   33.6       138300455           1     138300455.0       138300455       138300455  cudaDeviceSynchronize                                                           \n",
      "    5.5        22774277           3       7591425.7         6396914         8250642  cudaFree                                                                        \n",
      "    0.0           59618           1         59618.0           59618           59618  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       138285266           1     138285266.0       138285266       138285266  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.3        76816704        6689         11484.0            2176          173568  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   21.7        21274240         768         27700.8            1600          170144  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            6689               58.785              4.000              992.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   53.5      1328780143          72      18455279.8           21209       100134363  poll                                                                            \n",
      "   41.7      1034660543          72      14370285.3           16306       100227932  sem_timedwait                                                                   \n",
      "    3.6        90578458         663        136619.1            1102        17518235  ioctl                                                                           \n",
      "    1.0        25302764          91        278052.4            1487         8081065  mmap                                                                            \n",
      "    0.1         1508359          77         19589.1            4614           36768  open64                                                                          \n",
      "    0.0          184986           4         46246.5           36253           50083  pthread_create                                                                  \n",
      "    0.0          161156           3         53718.7           51168           57988  fgets                                                                           \n",
      "    0.0          134693          18          7482.9            1000          114556  fcntl                                                                           \n",
      "    0.0          129845          25          5193.8            1396           24425  fopen                                                                           \n",
      "    0.0          103780          11          9434.5            2473           21149  write                                                                           \n",
      "    0.0           89492           6         14915.3           11508           21064  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           64216          14          4586.9            1975            8089  munmap                                                                          \n",
      "    0.0           37315           5          7463.0            3079           10660  open                                                                            \n",
      "    0.0           28783          13          2214.1            1380            3644  read                                                                            \n",
      "    0.0           25534          17          1502.0            1104            3993  fclose                                                                          \n",
      "    0.0           16776           2          8388.0            6751           10025  socket                                                                          \n",
      "    0.0            9891           4          2472.7            2017            2940  mprotect                                                                        \n",
      "    0.0            9030           3          3010.0            1575            4142  fread                                                                           \n",
      "    0.0            8408           1          8408.0            8408            8408  pipe2                                                                           \n",
      "    0.0            8011           1          8011.0            8011            8011  connect                                                                         \n",
      "    0.0            7970           1          7970.0            7970            7970  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            2835           1          2835.0            2835            2835  bind                                                                            \n",
      "    0.0            1738           1          1738.0            1738            1738  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report14.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report14.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-ad8c-63e8-8162-a0e0.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-ad8c-63e8-8162-a0e0.qdrep\"\n",
      "Exporting 1912 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-ad8c-63e8-8162-a0e0.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.4       277076273           3      92358757.7           23905       276960932  cudaMallocManaged                                                               \n",
      "   15.8        55716498           1      55716498.0        55716498        55716498  cudaDeviceSynchronize                                                           \n",
      "    5.8        20320702           3       6773567.3         5934858         8301332  cudaFree                                                                        \n",
      "    0.0           93389           4         23347.2            7692           64321  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   96.7        53879800           3      17959933.3        17430729        18634575  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "    3.3         1853817           1       1853817.0         1853817         1853817  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21337856         768         27783.7            1600          181120  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   46.5       725404259          37      19605520.5           22654       101955123  poll                                                                            \n",
      "   44.6       696050624          37      18812179.0           17572       100127470  sem_timedwait                                                                   \n",
      "    7.1       111174080         667        166677.8            1090        32525365  ioctl                                                                           \n",
      "    1.6        24227211          91        266233.1            1765         8212144  mmap                                                                            \n",
      "    0.1         1992389          77         25875.2           10216           62377  open64                                                                          \n",
      "    0.0          374534           3        124844.7          116782          131774  fgets                                                                           \n",
      "    0.0          366650           4         91662.5           61393          103652  pthread_create                                                                  \n",
      "    0.0          196862          25          7874.5            2943           26672  fopen                                                                           \n",
      "    0.0          118616          11         10783.3            4041           27547  write                                                                           \n",
      "    0.0           73942          13          5687.8            2147           10704  munmap                                                                          \n",
      "    0.0           72924          48          1519.3            1017            6220  fcntl                                                                           \n",
      "    0.0           70401           5         14080.2            6613           22604  open                                                                            \n",
      "    0.0           60851           5         12170.2            1160           22049  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           42041          18          2335.6            1439            5809  fclose                                                                          \n",
      "    0.0           38895           1         38895.0           38895           38895  sem_wait                                                                        \n",
      "    0.0           36325           2         18162.5           17919           18406  socket                                                                          \n",
      "    0.0           33286          13          2560.5            1331            5066  read                                                                            \n",
      "    0.0           25076           3          8358.7            2806           14876  fread                                                                           \n",
      "    0.0           20472           4          5118.0            3853            6089  mprotect                                                                        \n",
      "    0.0           16669           1         16669.0           16669           16669  connect                                                                         \n",
      "    0.0            7143           1          7143.0            7143            7143  pipe2                                                                           \n",
      "    0.0            7012           1          7012.0            7012            7012  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            6595           1          6595.0            6595            6595  bind                                                                            \n",
      "    0.0            3435           1          3435.0            3435            3435  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report15.qdrep\"\n",
      "Report file moved to \"/dli/task/report15.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-c05f-6b40-ada8-fec9.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-c05f-6b40-ada8-fec9.qdrep\"\n",
      "Exporting 1870 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-c05f-6b40-ada8-fec9.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   88.9       218002799           3      72667599.7           18149       217932482  cudaMallocManaged                                                               \n",
      "    8.2        20147855           3       6715951.7         1046046        17943584  cudaFree                                                                        \n",
      "    2.3         5551848           1       5551848.0         5551848         5551848  cudaDeviceSynchronize                                                           \n",
      "    0.6         1541559           3        513853.0          122898          773035  cudaMemPrefetchAsync                                                            \n",
      "    0.0           62821           4         15705.3            5669           42850  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   52.6         2068824           3        689608.0          682483          702898  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "   47.4         1861692           1       1861692.0         1861692         1861692  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21204672         768         27610.3            1632          159968  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   48.4       581495014          34      17102794.5           17889       100132795  poll                                                                            \n",
      "   42.8       514174899          32      16067965.6            9401       100071829  sem_timedwait                                                                   \n",
      "    6.6        79506925         669        118844.4            1047        17353965  ioctl                                                                           \n",
      "    1.9        22438214          92        243893.6            1330        17867219  mmap                                                                            \n",
      "    0.2         1808726          77         23489.9            5075           40027  open64                                                                          \n",
      "    0.0          243308           5         48661.6           32760           63883  pthread_create                                                                  \n",
      "    0.0          161359           3         53786.3           50973           58818  fgets                                                                           \n",
      "    0.0          135001          25          5400.0            1482           24323  fopen                                                                           \n",
      "    0.0          102447          12          8537.2            4043           17073  write                                                                           \n",
      "    0.0           92315           2         46157.5           39679           52636  sem_wait                                                                        \n",
      "    0.0           58717          13          4516.7            1874            8295  munmap                                                                          \n",
      "    0.0           36466           5          7293.2            3288            9694  open                                                                            \n",
      "    0.0           28404          18          1578.0            1003            4985  fclose                                                                          \n",
      "    0.0           20324          10          2032.4            1480            3079  read                                                                            \n",
      "    0.0           18675          12          1556.3            1034            4943  fcntl                                                                           \n",
      "    0.0           18532           2          9266.0            7793           10739  socket                                                                          \n",
      "    0.0           15783           1         15783.0           15783           15783  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0           11237           5          2247.4            1769            2584  mprotect                                                                        \n",
      "    0.0           10955           3          3651.7            2204            4979  fread                                                                           \n",
      "    0.0            9190           1          9190.0            9190            9190  connect                                                                         \n",
      "    0.0            7318           1          7318.0            7318            7318  pipe2                                                                           \n",
      "    0.0            2711           1          2711.0            2711            2711  bind                                                                            \n",
      "    0.0            1974           1          1974.0            1974            1974  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report20.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report20.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-cd24-43df-8b32-f057.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-cd24-43df-8b32-f057.qdrep\"\n",
      "Exporting 1168 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-cd24-43df-8b32-f057.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.9       218283660           3      72761220.0           18056       218197825  cudaMallocManaged                                                               \n",
      "   15.0        41398492           4      10349623.0          119737        39870342  cudaMemPrefetchAsync                                                            \n",
      "    4.3        11870267           3       3956755.7          795813        10160533  cudaFree                                                                        \n",
      "    1.8         4887941           1       4887941.0         4887941         4887941  cudaDeviceSynchronize                                                           \n",
      "    0.0           98065           4         24516.2           10955           63136  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   52.5         2047766           3        682588.7          674866          688978  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "   47.5         1852987           1       1852987.0         1852987         1852987  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20433760          64        319277.5          319040          319488  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000              64             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.6       438372859          27      16236031.8           20552       100085168  sem_timedwait                                                                   \n",
      "   43.0       432208543          28      15436019.4           29956       100140066  poll                                                                            \n",
      "   11.8       118686227         668        177674.0            1042        39805332  ioctl                                                                           \n",
      "    1.4        14283867          92        155259.4            1396        10093909  mmap                                                                            \n",
      "    0.2         1520951          77         19752.6            5044           39729  open64                                                                          \n",
      "    0.0          273617           5         54723.4           43555           65395  pthread_create                                                                  \n",
      "    0.0          163347          25          6533.9            1770           33839  fopen                                                                           \n",
      "    0.0          162329           3         54109.7           51557           58270  fgets                                                                           \n",
      "    0.0          115145           3         38381.7            1083           79856  sem_wait                                                                        \n",
      "    0.0          104265          12          8688.7            4084           15363  write                                                                           \n",
      "    0.0           58008          14          4143.4            1954            8093  munmap                                                                          \n",
      "    0.0           45319           5          9063.8            3703           14098  open                                                                            \n",
      "    0.0           44197           4         11049.2            1041           19763  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           37852          14          2703.7            1307            5139  read                                                                            \n",
      "    0.0           31144          18          1730.2            1034            4819  fclose                                                                          \n",
      "    0.0           22603          15          1506.9            1020            5011  fcntl                                                                           \n",
      "    0.0           18965           2          9482.5            7193           11772  socket                                                                          \n",
      "    0.0           12956           5          2591.2            2150            3173  mprotect                                                                        \n",
      "    0.0           11631           1         11631.0           11631           11631  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0           10817           3          3605.7            1911            5163  fread                                                                           \n",
      "    0.0            9516           1          9516.0            9516            9516  pipe2                                                                           \n",
      "    0.0            8521           1          8521.0            8521            8521  connect                                                                         \n",
      "    0.0            3190           1          3190.0            3190            3190  bind                                                                            \n",
      "    0.0            1612           1          1612.0            1612            1612  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report23.qdrep\"\n",
      "Report file moved to \"/dli/task/report23.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](../edit/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nsys profile`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *300us*. Check out [the solution](../edit/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-saxpy/01-saxpy.cu(31): warning: variable \"numberOfSMs\" was declared but never referenced\n",
      "\n",
      "c[0] = 1, c[1] = 1, c[2] = 1, c[3] = 1, c[4] = 1, \n",
      "c[4194299] = 1, c[4194300] = 1, c[4194301] = 1, c[4194302] = 1, c[4194303] = 1, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "c[0] = 1, c[1] = 1, c[2] = 1, c[3] = 1, c[4] = 1, \n",
      "c[4194299] = 1, c[4194300] = 1, c[4194301] = 1, c[4194302] = 1, c[4194303] = 1, \n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-4064-fa57-93df-1788.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-4064-fa57-93df-1788.qdrep\"\n",
      "Exporting 1094 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-4064-fa57-93df-1788.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   96.6       241403505           3      80467835.0           29546       241318555  cudaMallocManaged                                                               \n",
      "    2.2         5551451           4       1387862.7           67275         5059838  cudaMemPrefetchAsync                                                            \n",
      "    0.8         2078242           3        692747.3          177457         1702383  cudaFree                                                                        \n",
      "    0.4          892110           1        892110.0          892110          892110  cudaDeviceSynchronize                                                           \n",
      "    0.0           89155           4         22288.8            9413           55416  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   57.2          271195           3         90398.3           90334           90463  initWith(int, int*)                                                                                                                                                                                                                                                                                                                          \n",
      "   42.8          203324           1        203324.0          203324          203324  saxpy(int*, int*, int*)                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0         2553952           8        319244.0          319072          319360  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "          16384.000               8             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   47.0       331566556          16      20722909.7           12169       100080623  sem_timedwait                                                                   \n",
      "   39.9       281438514          18      15635473.0           22382       100151301  poll                                                                            \n",
      "   12.1        85150561         656        129802.7            1011        18287550  ioctl                                                                           \n",
      "    0.7         4970808          91         54624.3            1260         1662032  mmap                                                                            \n",
      "    0.2         1655118          77         21495.0            4778           43576  open64                                                                          \n",
      "    0.0          255035           5         51007.0           35568           60005  pthread_create                                                                  \n",
      "    0.0          161649           3         53883.0           51147           57891  fgets                                                                           \n",
      "    0.0          134130          25          5365.2            1845           24623  fopen                                                                           \n",
      "    0.0           94824          12          7902.0            3789           12469  write                                                                           \n",
      "    0.0           70346           2         35173.0           26842           43504  sem_wait                                                                        \n",
      "    0.0           54924          12          4577.0            2162            8244  munmap                                                                          \n",
      "    0.0           48473          37          1310.1            1003            4754  fcntl                                                                           \n",
      "    0.0           39017           5          7803.4            3059           10982  open                                                                            \n",
      "    0.0           36707           3         12235.7           11544           12623  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           27750          18          1541.7            1002            5057  fclose                                                                          \n",
      "    0.0           23119          12          1926.6            1264            2827  read                                                                            \n",
      "    0.0           18263           2          9131.5            7395           10868  socket                                                                          \n",
      "    0.0           10796           3          3598.7            2089            4767  fread                                                                           \n",
      "    0.0           10510           5          2102.0            1774            2291  mprotect                                                                        \n",
      "    0.0            9468           1          9468.0            9468            9468  connect                                                                         \n",
      "    0.0            7415           1          7415.0            7415            7415  pipe2                                                                           \n",
      "    0.0            6570           1          6570.0            6570            6570  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            2431           1          2431.0            2431            2431  bind                                                                            \n",
      "    0.0            1856           1          1856.0            1856            1856  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report26.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report26.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download error handling wrapper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
